# Pipeline de Predi√ß√£o de Sal√°rio - Documenta√ß√£o 06/11/2024 - v1.1

**Elias Andrade - Arquiteto de Solu√ß√µes Replika AI Solutions - Maring√°, PR**

Este documento descreve o pipeline de predi√ß√£o de sal√°rio, um sistema de processamento de dados em tempo real que utiliza machine learning para prever o sal√°rio de um indiv√≠duo com base em sua idade.  Este projeto √© uma jornada rumo √† inova√ß√£o, como a busca pela pedra filosofal dos alquimistas.  Assim como os alquimistas buscavam a transmuta√ß√£o de metais, n√≥s buscamos a transmuta√ß√£o de dados em conhecimento.

## Vis√£o Geral 

üöÄ Este pipeline utiliza uma arquitetura de microsservi√ßos, inspirada na eleg√¢ncia e efici√™ncia da natureza, como a intrincada rede de um formigueiro.  Cada componente opera de forma independente, permitindo escalabilidade e flexibilidade.  A modularidade permite que cada parte seja atualizada e melhorada sem afetar o funcionamento do sistema como um todo.

![Arquitetura do Pipeline](arquitetura.png)  **(Diagrama a ser adicionado)**

## Componentes Principais

* **Gerador de Dados (`gerador_stream.py`):** üè≠ Gera dados sint√©ticos, simulando informa√ß√µes realistas de indiv√≠duos.  A inspira√ß√£o veio da capacidade de cria√ß√£o e inova√ß√£o da natureza, que constantemente gera novas formas de vida.  Este componente √© configur√°vel atrav√©s das vari√°veis de ambiente `TAMANHO_BUFFER` e `INTERVALO_GERACAO`.

* **Normalizador de Dados (`normalizador_stream.py`):** üîÑ Normaliza os dados usando `StandardScaler` para melhorar a performance do modelo de machine learning.  A inspira√ß√£o veio da busca pela harmonia e equil√≠brio, como a busca pela perfei√ß√£o na arte.  Este componente √© configur√°vel atrav√©s da vari√°vel de ambiente `INTERVALO_NORMALIZACAO`.

* **Treinador de Modelo (`treinador_stream.py`):** üß† Treina um modelo de regress√£o (`RandomForestRegressor`) para prever o sal√°rio com base na idade.  A inspira√ß√£o veio da capacidade do c√©rebro humano de aprender e se adaptar a novas informa√ß√µes, como a capacidade de um mestre artes√£o de aprimorar suas habilidades ao longo do tempo.  Este componente √© configur√°vel atrav√©s da vari√°vel de ambiente `INTERVALO_TREINAMENTO`.  Os modelos treinados s√£o salvos na pasta `modelos`.

* **Consumidor de Previs√µes (`consumidor_stream.py`):** ü§ñ Consome as previs√µes do modelo treinado e as utiliza para outras aplica√ß√µes.  A inspira√ß√£o veio da capacidade das m√°quinas de executar tarefas repetitivas com precis√£o e efici√™ncia, como uma linha de montagem que produz produtos em s√©rie.  Este componente faz requisi√ß√µes a cada 60 segundos √† API do treinador.

* **Lan√ßador do Pipeline (`stream_pipeline_launcher.py`):** üïπÔ∏è Inicia e monitora todos os servi√ßos do pipeline, garantindo a execu√ß√£o cont√≠nua.  A inspira√ß√£o veio da capacidade de orquestra√ß√£o e controle de um maestro em uma orquestra.  Este componente utiliza logs detalhados para monitoramento e tratamento de erros.


## Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o principal, escolhida por sua versatilidade e ampla comunidade.  üêç
* **FastAPI:** Framework para cria√ß√£o de APIs RESTful, conhecido por sua velocidade e facilidade de uso. üöÄ
* **Scikit-learn:** Biblioteca para machine learning, oferecendo algoritmos robustos e eficientes. ü§ñ
* **Pandas:** Biblioteca para manipula√ß√£o de dados, facilitando o processamento e an√°lise de dados. üêº
* **Faker:** Biblioteca para gera√ß√£o de dados sint√©ticos, permitindo a simula√ß√£o de dados realistas. üé≠


## Pr√≥ximos Passos

* Adicionar m√©tricas de performance ao modelo, como RMSE, R¬≤, e MAE. üìä
* Implementar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo. üß™
* Criar uma interface gr√°fica para monitorar o pipeline em tempo real. üñ•Ô∏è
* Explorar outras t√©cnicas de machine learning, como redes neurais, para melhorar a precis√£o do modelo. üß†
* Implementar um sistema de versionamento mais robusto para os modelos treinados. üíæ


## Refer√™ncias

* [Scikit-learn](https://scikit-learn.org/stable/)
* [FastAPI](https://fastapi.tiangolo.com/)
* [Pandas](https://pandas.pydata.org/)
* [Faker](https://faker.readthedocs.io/en/master/)


# Sistema de Processamento de Stream de Dados - Pipeline de Predi√ß√£o de Sal√°rio

**üöÄ Projeto:** Pipeline de Predi√ß√£o de Sal√°rio em Tempo Real

**üë®‚Äçüíª Desenvolvedor:** Elias Andrade - Arquiteto de Solu√ß√µes Replika AI Solutions - Maring√°, PR - 06/11/2024

[![Python Version](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/fastapi-0.95.2-green.svg)](https://fastapi.tiangolo.com/)
[![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.3.0-orange.svg)](https://scikit-learn.org/stable/)
[![License](https://img.shields.io/badge/license-MIT-lightgrey.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](test_results.txt)


---

## Vis√£o Geral

Este projeto demonstra um pipeline completo de processamento de stream de dados em tempo real, constru√≠do com microservi√ßos FastAPI e utilizando Python.  Como afirma a documenta√ß√£o oficial do FastAPI ("FastAPI: Modern, Fast (high-performance), Web framework for building APIs with Python 3.7+ based on standard Python type hints"), a escolha do FastAPI se justifica pela sua velocidade e facilidade de uso. O pipeline consiste em quatro componentes principais, cada um executando como um microsservi√ßo independente, comunicando-se via requisi√ß√µes HTTP:

* **Gerador de Dados:** Gera dados sint√©ticos para simular um fluxo cont√≠nuo de informa√ß√µes.
* **Normalizador:** Normaliza os dados para garantir consist√™ncia no treinamento do modelo.
* **Treinador:** Treina um modelo de Machine Learning para prever o sal√°rio com base na idade e outras vari√°veis.
* **Consumidor:** Consome as previs√µes do modelo treinado, simulando um sistema que utiliza as previs√µes em tempo real.


---

## Arquitetura

O pipeline segue uma arquitetura modular e escal√°vel, baseada em microservi√ßos.  A comunica√ß√£o ass√≠ncrona entre os componentes √© feita atrav√©s de requisi√ß√µes HTTP, permitindo alta disponibilidade e escalabilidade horizontal.  Esta abordagem √© consistente com as melhores pr√°ticas de desenvolvimento de microsservi√ßos, como descrito em "Building Microservices" de Sam Newman.

```
+-----------------+     +-----------------+     +-----------------+     +-----------------+
| Gerador de Dados |---->| Normalizador    |---->| Treinador      |---->| Consumidor      |
+-----------------+     +-----------------+     +-----------------+     +-----------------+
     ^                                                                        |
     |                                                                        v
     +-----------------------------------------------------------------------+
                                         |
                                         v
                                  Monitoramento e Logs (Utilizando Loguru)
```

**[Diagrama de Arquitetura com √≠cones UML]** (Incluir um diagrama de arquitetura com √≠cones UML representando cada componente e suas intera√ß√µes)


---

## Componentes

### 1. Gerador de Dados (`gerador_stream.py`)

üè≠ **Funcionalidade:** Gera dados sint√©ticos usando a biblioteca `Faker`, simulando um fluxo cont√≠nuo de informa√ß√µes, incluindo idade, experi√™ncia profissional, n√≠vel de educa√ß√£o e outras vari√°veis relevantes para a predi√ß√£o de sal√°rio.  A utiliza√ß√£o do Faker simplifica o processo de gera√ß√£o de dados de teste, conforme descrito na sua documenta√ß√£o: [Faker Documentation](https://faker.readthedocs.io/en/master/).

‚öôÔ∏è **Configura√ß√µes (Vari√°veis de Ambiente):**

* `TAMANHO_BUFFER`: Tamanho do buffer de dados gerados (default: 1000).
* `INTERVALO_GERACAO`: Intervalo de gera√ß√£o de dados em segundos (default: 5).
* `NUM_VARI√ÅVEIS`: N√∫mero de vari√°veis a serem geradas (default: 5).

**Exemplo de Dados Gerados:**

```json
[
  {"idade": 30, "experiencia": 5, "educacao": "Gradua√ß√£o", ...},
  {"idade": 25, "experiencia": 2, "educacao": "T√©cnico", ...},
  ...
]
```

**C√≥digo Exemplo (trecho):**

```python
from faker import Faker
fake = Faker('pt_BR')
# ... restante do c√≥digo ...
```

üîó **Refer√™ncia:** [Faker](https://faker.readthedocs.io/en/master/)


---

### 2. Normalizador (`normalizador_stream.py`)

üîÑ **Funcionalidade:** Normaliza os dados num√©ricos usando `StandardScaler` do scikit-learn, garantindo que todas as features tenham a mesma escala, evitando que features com valores maiores dominem o processo de treinamento.  A escolha do `StandardScaler` √© justificada pela sua efic√°cia em normalizar dados com distribui√ß√£o aproximadamente normal, conforme explicado na documenta√ß√£o do scikit-learn: [StandardScaler Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).

‚öôÔ∏è **Configura√ß√µes (Vari√°veis de Ambiente):**

* `INTERVALO_NORMALIZACAO`: Intervalo de normaliza√ß√£o em segundos (default: 10).
* `VARI√ÅVEIS_NORMALIZAR`: Lista de vari√°veis a serem normalizadas (default: ["idade", "experiencia"]).

**C√≥digo Exemplo (trecho):**

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# ... restante do c√≥digo ...
```

üîó **Refer√™ncia:** [Scikit-learn](https://scikit-learn.org/stable/)


---

### 3. Treinador (`treinador_stream.py`)

üß† **Funcionalidade:** Treina um modelo `RandomForestRegressor` periodicamente com os dados normalizados, salvando o modelo treinado em um arquivo `.joblib`.  Implementa um sistema de versionamento de modelos, mantendo os 3 √∫ltimos modelos treinados.  O `RandomForestRegressor` foi escolhido por sua robustez e capacidade de lidar com dados n√£o lineares, conforme descrito na documenta√ß√£o: [RandomForestRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).

‚öôÔ∏è **Configura√ß√µes (Vari√°veis de Ambiente):**

* `INTERVALO_TREINAMENTO`: Intervalo de treinamento em segundos (default: 60).
* `MODELO_PATH`: Caminho para salvar os modelos treinados (default: "modelos/").

**C√≥digo Exemplo (trecho):**

```python
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
# ... restante do c√≥digo ...
```

üíæ **Sa√≠da:** Modelos treinados salvos na pasta `modelos` com nomes baseados em timestamps.

üîó **Refer√™ncia:** [Scikit-learn](https://scikit-learn.org/stable/), [Joblib](https://joblib.readthedocs.io/en/latest/)


---

### 4. Consumidor (`consumidor_stream.py`)

ü§ñ **Funcionalidade:** Consome previs√µes do Treinador via requisi√ß√µes HTTP, simulando um sistema que utiliza as previs√µes em tempo real.  Inclui tratamento de erros e logging detalhado.  A utiliza√ß√£o de requisi√ß√µes HTTP para comunica√ß√£o entre microsservi√ßos √© uma pr√°tica comum e bem documentada, como descrito em: [REST API Design Best Practices](https://www.restapitutorial.com/lessons/restfulbestpractices.html).

‚öôÔ∏è **Configura√ß√µes (Vari√°veis de Ambiente):**

* `TREINADOR_URL`: URL da API do Treinador (default: "http://localhost:8001/predict").
* `INTERVALO_CONSUMO`: Intervalo de consumo em segundos (default: 15).

**C√≥digo Exemplo (trecho):**

```python
import requests
response = requests.post(TREINADOR_URL, json=data)
# ... restante do c√≥digo ...
```

üîó **Refer√™ncia:** [Requests](https://requests.readthedocs.io/en/master/)


---

## Tecnologias

* **Python 3.9+** üêç  A escolha do Python se deve √† sua vasta biblioteca de ferramentas para ci√™ncia de dados e desenvolvimento web.
* **FastAPI** ‚ö°Ô∏è [![FastAPI](https://img.shields.io/badge/fastapi-0.95.2-green.svg)](https://fastapi.tiangolo.com/)  FastAPI √© um framework moderno e de alto desempenho para constru√ß√£o de APIs em Python.  Sua base em type hints permite a gera√ß√£o autom√°tica de documenta√ß√£o e valida√ß√£o de dados.
* **Scikit-learn** ü§ñ [![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.3.0-orange.svg)](https://scikit-learn.org/stable/)  Scikit-learn √© uma biblioteca poderosa para Machine Learning em Python, fornecendo algoritmos de regress√£o, classifica√ß√£o e clustering.
* **Faker** üé≠ [![Faker](https://img.shields.io/badge/faker-10.1.0-blue.svg)](https://faker.readthedocs.io/en/master/)  Faker √© uma biblioteca para gerar dados sint√©ticos realistas, √∫til para testes e simula√ß√£o.
* **Joblib** üíæ [![Joblib](https://img.shields.io/badge/joblib-1.2.0-purple.svg)](https://joblib.readthedocs.io/en/latest/)  Joblib √© uma biblioteca para persistir modelos de Machine Learning de forma eficiente.
* **Requests** üåê [![Requests](https://img.shields.io/badge/requests-2.31.0-brightgreen.svg)](https://requests.readthedocs.io/en/master/)  Requests √© uma biblioteca para fazer requisi√ß√µes HTTP em Python.
* **Asyncio** ÂºÇÊ≠•  Asyncio permite a constru√ß√£o de aplica√ß√µes concorrentes e ass√≠ncronas em Python.
* **Loguru** ü™µ [![Loguru](https://img.shields.io/badge/loguru-0.7.0-yellow.svg)](https://loguru.readthedocs.io/en/stable/)  Loguru √© uma biblioteca de logging moderna e eficiente para Python.
* **Pydantic** üõ°Ô∏è [![Pydantic](https://img.shields.io/badge/pydantic-2.4.2-red.svg)](https://pydantic-docs.helpmanual.io/)  Pydantic √© uma biblioteca para valida√ß√£o e parse de dados em Python.


---

## Como Executar

1. **Instalar depend√™ncias:** `pip install -r requirements.txt`
2. **Configurar vari√°veis de ambiente:**  Definir as vari√°veis de ambiente para cada componente (ver documenta√ß√£o individual).
3. **Executar cada componente em um terminal separado:**  Utilizar comandos `uvicorn` para cada microsservi√ßo (ex: `uvicorn gerador_stream:app --reload`).


---

## Monitoramento e Logs

O pipeline utiliza a biblioteca `Loguru` para logging.  Os logs s√£o gravados em arquivos separados para cada componente, facilitando o monitoramento e a depura√ß√£o.  Um sistema de monitoramento centralizado poderia ser implementado futuramente para agregar e visualizar os logs de todos os componentes.  A escolha do Loguru se deve √† sua facilidade de uso e flexibilidade, conforme descrito em sua documenta√ß√£o: [Loguru Documentation](https://loguru.readthedocs.io/en/stable/).

**Exemplo de Log (Loguru):**

```
2024-11-07 10:00:00.000 | INFO     | gerador_stream: Dados gerados com sucesso. Buffer size: 1000.
```


---

## Testes

O projeto inclui testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo.  Os testes s√£o executados usando o framework `pytest`.

**Comando para executar os testes:** `pytest`

**Relat√≥rio de Testes:** [test_results.txt](test_results.txt)


---

## Implanta√ß√£o

O pipeline pode ser implantado em um ambiente de cont√™ineres (Docker) para facilitar a portabilidade e a escalabilidade.  Um arquivo `docker-compose.yml` poderia ser criado para orquestrar a implanta√ß√£o dos microservi√ßos.


---

## Seguran√ßa

Considera√ß√µes de seguran√ßa incluem:

* **Autentica√ß√£o e Autoriza√ß√£o:** Implementa√ß√£o de mecanismos de autentica√ß√£o e autoriza√ß√£o para proteger as APIs dos microservi√ßos.
* **Tratamento de Erros:**  Tratamento robusto de erros para evitar vulnerabilidades e garantir a estabilidade do sistema.
* **Valida√ß√£o de Dados:** Valida√ß√£o rigorosa dos dados de entrada para prevenir ataques de inje√ß√£o.


---

## Considera√ß√µes Futuras

* **Escalabilidade:** Integra√ß√£o com o Kafka para melhor escalabilidade e processamento de grandes volumes de dados.
* **Persist√™ncia de Dados:** Utiliza√ß√£o de um banco de dados NoSQL (ex: MongoDB) para persistir os dados gerados e as previs√µes.
* **Monitoramento:** Implementa√ß√£o de um sistema de monitoramento mais robusto, com dashboards e alertas em tempo real.
* **CI/CD:** Implementa√ß√£o de um pipeline de CI/CD para automatizar o processo de integra√ß√£o cont√≠nua e entrega cont√≠nua.
* **Interface Gr√°fica:** Desenvolvimento de uma interface gr√°fica para monitorar o pipeline e visualizar os dados.
* **Modelagem Alternativa:** Explora√ß√£o de outros modelos de Machine Learning para melhorar a precis√£o das previs√µes.
* **Testes:** Expans√£o da cobertura de testes para incluir testes de desempenho e testes de seguran√ßa.


---

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.


---

**[Conclus√£o com emojis e √≠cones]** üéâ  Este projeto demonstra um pipeline de processamento de dados robusto e escal√°vel, pronto para lidar com grandes volumes de dados em tempo real.  Sua arquitetura modular e o uso de tecnologias modernas garantem alta disponibilidade e facilidade de manuten√ß√£o. üöÄ


